# 0831 TIL - MapReduce Phase ì½”ë“œ ë¶„ì„
## 1. ProgramDriver

ProgramDriverëŠ” êµ¬í˜„í•œ MapReduce Phaseë¥¼ `addClass`ë©”ì„œë“œë¥¼ ì´ìš©í•´ jaríŒŒì¼ì— ë“±ë¡í•˜ëŠ” ì—­í• ì„ í•˜ê³ , `driver`ë©”ì„œë“œë¥¼ í†µí•´ í•´ë‹¹ í´ë˜ìŠ¤ì˜ `main`ë©”ì„œë“œë¥¼ ì‹¤í–‰í•œë‹¤.

---

### 1.1) addClass(String, Class<?>, String)

ê°ê°ì˜ parameterëŠ” **name, mainClass, description** ìˆœìœ¼ë¡œ ì‚¬ìš©ë˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.

- **name** : CLI í™˜ê²½ì—ì„œ ì‚¬ìš©ë  jar íŒŒì¼ì— ë“±ë¡í•  mainClassì˜ alias
- **mainClass** : Map, Reduce í•¨ìˆ˜ ë“± jar íŒŒì¼ì— ë“±ë¡í•  MapReduce Phaseë¥¼ ì •ì˜í•œ class
- **description** : MapReduce Phaseì— ëŒ€í•œ ì„¤ëª…

---

### 1.2) driver(String[])

parameter ë°°ì—´ì˜ ì²«ë²ˆì§¸ ë¬¸ìì—´ì€ hadoopì—ì„œ ì‹¤í–‰í•  programì˜ ì´ë¦„ì— í•´ë‹¹í•œë‹¤. ì¦‰, `addClass`ë©”ì„œë“œì˜ name parameterì™€ ë™ì¼í•´ì•¼ í•œë‹¤. ë§Œì•½ í•´ë‹¹ programì´ ì¡´ì¬í•œë‹¤ë©´, ë°°ì—´ì˜ ë‚¨ì€ ë¬¸ìì—´ë“¤ì„ parameterë¡œ í•˜ì—¬ programì˜ `main`ë©”ì„œë“œë¥¼ ì‹¤í–‰í•œë‹¤.

```java
import org.apache.hadoop.util.ProgramDriver;

public class Driver {
    
    public static void main(String[] args) {
        
        int exitCode = -1;
        ProgramDriver programDriver = new ProgramDriver();
        try {
            programDriver.addClass("mapReducePhase", MapReducePhase.class, 
								"A map/reduce program");
            programDriver.driver(args);
            exitCode = 0;
        } catch (Throwable e) {
            e.printStackTrace();
        }

        System.exit(exitCode);
    }
}
```

---

## 2. MapReduce Phase

### 2.1) main ë©”ì„œë“œ

MapReduce Phaseì˜ **Main í•¨ìˆ˜**ì— í•´ë‹¹í•˜ë©° ì„¤ì •ì„ ë‹´ë‹¹í•œë‹¤.

- main ì½”ë“œ ì˜ˆì‹œ

```java
public static void main(String[] args) throws Exception {
    Configuration configuration = new Configuration();
    String[] remainingArgs = new GenericOptionsParser(configuration, args).getRemainingArgs();
        
    if (remainingArgs.length != 2) {
        System.err.println("Usage: <in> <out>");
        System.exit(2);
    }
    deleteOutputIfExists(configuration, remainingArgs[1]);

    Job job = Job.getInstance(configuration, "MapReduce Phase");
    setJob(job);
    setInputAndOutputPath(job, remainingArgs);

    System.exit(job.waitForCompletion(true) ? 0 : 1);
}

private final static void deleteOutputIfExists(final Configuration configuration, final String outputPath) {
    FileSystem hdfs = FileSystem.get(configuration);
    Path output = new Path(outputPath);
    if (hdfs.exists(output)) {
        hdfs.delete(output, false);
    }
}

private final static void setJob(final Job job) {
    // JaríŒŒì¼ì— ë“±ë¡í•  MapReduce í´ë˜ìŠ¤ ì„¤ì •(Main í•¨ìˆ˜ë¥¼ í¬í•¨í•œ í´ë˜ìŠ¤)
    job.setJarByClass(MapReducePhase.class);

    // Mapper, Combiner, Partitioner, Reducerë¥¼ ë‹´ë‹¹í•  í´ë˜ìŠ¤ ì„¤ì •
    job.setMapperClass(MyMapper.class);
    job.setCombinerClass(MyReducer.class);
    job.setPartitionerClass(MyPartitioner.class);
    job.setReducerClass(MyReducer.class);

    // Reducerì˜ ê²°ê³¼ë¡œ ì¶œë ¥í•  (Key, Value) íƒ€ì… ì„¤ì •
    // Mapê³¼ Reducerì˜ ê²°ê³¼ íƒ€ì…ì´ ê°™ì„ ê²½ìš° Mapì€ ê²°ê³¼ íƒ€ì… ì„¤ì • ìƒëµ ê°€ëŠ¥
    job.setOutputKeyClass(Text.class);
    job.setOutputValueClass(IntWritable.class);

    // Reducerë¡œ ë™ì‘í•  Machine ìˆ˜ ì„¤ì •
    job.setNumReduceTasks(2);
}

private final static void setInputAndOutputPath(final Job job, final String[] remainingArgs) throws IOException {
    FileInputFormat.addInputPath(job, new Path(remainingArgs[0]));
    FileOutputFormat.setOutputPath(job, new Path(remainingArgs[1]));
}
```

- `deleteOutputIfExists`ë©”ì„œë“œ

    ë§¤ë²ˆ jaríŒŒì¼ì„ ìƒì„±í•  ë•Œë§ˆë‹¤ ì´ì „ì— ìƒì„±í–ˆë˜ output HDFS íŒŒì¼ì„ ì‚­ì œí•´ì•¼ í•˜ë¯€ë¡œ, ì´ ì½”ë“œë¥¼ í†µí•´ ì´ì „ì— ìƒì„±ë˜ì—ˆë˜ output HDFS íŒŒì¼ì„ ì‚­ì œí•œë‹¤. ì œê³µëœ ì½”ë“œì—ì„œëŠ” íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ì½”ë“œê°€ `hdfs.delete(output);`ì´ì§€ë§Œ í•´ë‹¹ ë©”ì„œë“œëŠ” Deprecatedë˜ì—ˆìœ¼ë¯€ë¡œ Javadocì„ ì°¸ê³ í•˜ì—¬ ìœ„ì˜ ë©”ì„œë“œë¡œ ë³€ê²½í•˜ì˜€ë‹¤.

    Signatureì˜ `boolean`ê°’ì¸ recursiveê°€ ì˜ë¯¸í•˜ëŠ” ë°”ê°€ ë¬´ì—‡ì¸ì§€ ëª…í™•í•˜ì§€ ì•Šì•„ í™•ì¸í•´ ë³´ì•˜ë”ë‹ˆ **"pathê°€ ë””ë ‰í† ë¦¬ì´ê³  `true`ë¡œ ì„¤ì •ëœ ê²½ìš° ë””ë ‰í† ë¦¬ê°€ ì‚­ì œë˜ì§€ ì•Šìœ¼ë©´ ì˜ˆì™¸ê°€ ë°œìƒí•©ë‹ˆë‹¤. íŒŒì¼ì˜ ê²½ìš° recursiveëŠ” `true` ë˜ëŠ” `false`ë¡œ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."**ë¼ê³  í•œë‹¤. 
    (ë””ë ‰í† ë¦¬ë¥¼ ì‚­ì œí•  ê²½ìš° recursiveê°€ ì˜í–¥ì„ ì£¼ì§€ë§Œ, íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ê²½ìš° ì˜í–¥ì„ ì£¼ì§€ ì•ŠëŠ” ë“¯ í•˜ë‹¤.

- `setJob`ë©”ì„œë“œ

    Jobì„ ìƒì„±í•˜ëŠ” ì½”ë“œë„ ì£¼ì–´ì§„ ì½”ë“œì—ì„œëŠ”
    `Job job = new Job(...);`ì˜€ì§€ë§Œ Jobì˜ ìƒì„±ìê°€ Deprecatedë˜ì—ˆê¸° ë•Œë¬¸ì— Javadocì— ë”°ë¼ íŒ©í† ë¦¬ ë©”ì„œë“œì¸ `getInstance(...);`ë¥¼ ì‚¬ìš©í–ˆë‹¤.

- `setInputAndOutputPath`ë©”ì„œë“œ

    1.2)ì˜ `driver`ë©”ì„œë“œì—ì„œ CLI í™˜ê²½ì—ì„œ ì…ë ¥ë°›ì€ ì²«ë²ˆì§¸ ParameterëŠ” ì‹¤í–‰í•  programì˜ ì´ë¦„ì´ë¼ê³  í–ˆë‹¤. ë”°ë¼ì„œ CLI í™˜ê²½ì˜ ë‘ë²ˆì§¸ parameterë¡œ ì…ë ¥ë°›ì€ input HDFS íŒŒì¼ëª…ê³¼ ë‘ë²ˆì§¸ Parameterë¡œ ì…ë ¥ë°›ì€ output HDFS íŒŒì¼ëª…ì„ ì…ì¶œë ¥ íŒŒì¼ë¡œ ì„¤ì •í•œë‹¤.

---

### 2.2) Mapper<keyIn, valueIn, keyOut, valueOut>

MapReduceì˜ Map í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” classë¡œ, `import org.apache.hadoop.mapreduce.Mapper`ë¥¼ ìƒì†ë°›ì•„ `map`ë©”ì„œë“œë¥¼ `@Override`í•´ì•¼ í•œë‹¤.

```java
public static class MyMapper extends Mapper<Object, Text, Text, IntWritable> {

    private Text resultKey = new Text();
    private IntWritable resultValue = new IntWritable();
		
    @Override
    public void map(Object key, Text value, Context context) throws IOException, InterruptedException {

    String stringValue = value.toString();
    // ì•Œê³ ë¦¬ì¦˜ ìˆ˜í–‰ ê²°ê³¼ : String result;
    resultKey.set(result);
    context.write(resultKey, resultValue);

    }
}
```

---

### 2.3) Reducer<keyIn, valueIn, keyOut, valueOut>

MapReduceì˜ Reduce í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” classë¡œ, `import org.apache.hadoop.mapreduce.Reducer`ë¥¼ ìƒì†ë°›ì•„ `reduce`ë©”ì„œë“œë¥¼ `@Override`í•´ì•¼ í•œë‹¤. 

```java
public static class MyReducer extends Reducer<Text, IntWritable, Text, IntWritable> {

    private IntWritable resultValue = new IntWritable();

    public void reduce(Text key, Iterable<IntWritable> values, Context context) throws IOException, InterruptedException {

    int result = 0;
    for ( IntWritable value : values ) {
        int realValue = value.get();
        // ì•Œê³ ë¦¬ì¦˜ ìˆ˜í–‰
    }

    resultValue.set(result);
    context.write(key, resultValue);
    }
}
```

ğŸ“¢ `Reducer`ëŠ” Combine í•¨ìˆ˜ì— ì‚¬ìš©ë  ìˆ˜ ìˆë‹¤.

---

### 2.4) Partitioner<key, value>

PartitionerëŠ” Map í•¨ìˆ˜ì˜ outputì— ëŒ€í•´ Reduce í•¨ìˆ˜ë¥¼ ìˆ˜í–‰í•  Machineì„ íŠ¹ì • ì¡°ê±´ì— ë”°ë¼ ê²°ì •í•˜ëŠ” classë¡œ, `import org.apache.hadoop.mapreduce.Partitioner`ë¥¼ ìƒì†ë°›ì•„ `getPartition`ë©”ì„œë“œë¥¼ `@Override`í•´ì•¼ í•œë‹¤.
(ì•„ë˜ì˜ ì½”ë“œëŠ” keyê°’ì˜ ì²« ê¸€ìê°€ ASCII ì½”ë“œì˜ 'a'ë³´ë‹¤ ì‘ì„ ê²½ìš° 0ë²ˆ Machineìœ¼ë¡œ, í´ ê²½ìš° 1ë²ˆ Machineìœ¼ë¡œ ë°ì´í„°ë¥¼ Partitioní•œë‹¤)

```java
public static class MyPartitioner extends Partitioner<Text, IntWritable> {

    @Override
    public int getPartition(Text key, IntWritable value, int numPartitions) {
        if(key.toString().charAt(0) < 'a') {
            return 0;
        }
        return 1;
    }
}
```

---
